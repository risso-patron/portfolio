# ü§ñ GU√çA: Continue con IA Local (Sin Internet)

## üìã RESUMEN
Esta gu√≠a te ayudar√° a configurar Continue con Ollama para tener un asistente de c√≥digo que funciona **100% offline**.

---

## ‚úÖ ESTADO ACTUAL

- [x] Ollama instalado ‚úÖ
- [x] Continue instalado ‚úÖ
- [x] Configuraci√≥n creada ‚úÖ
- [ ] Modelo descargado ‚è≥
- [ ] Probado funcionamiento ‚è≥

---

## üöÄ PR√ìXIMOS PASOS

### **1. REINICIAR TU PC**
Ollama necesita un reinicio para funcionar correctamente.

```cmd
# Despu√©s del reinicio, verifica que Ollama est√© corriendo:
ollama --version
```

### **2. DESCARGAR UN MODELO LOCAL**

Elige seg√∫n tu RAM disponible:

#### **Opci√≥n A: Si tienes 8-12GB RAM** (Recomendado para empezar)
```cmd
ollama pull deepseek-coder:1.3b
```
- ‚úÖ R√°pido
- ‚úÖ Consume poca RAM (~2GB)
- ‚ö†Ô∏è Menos preciso

#### **Opci√≥n B: Si tienes 12-16GB RAM** (Mejor balance)
```cmd
ollama pull deepseek-coder:6.7b
```
- ‚úÖ Buena velocidad
- ‚úÖ Calidad decente
- ‚ö†Ô∏è Consume ~6GB RAM

#### **Opci√≥n C: Si tienes 16GB+ RAM** (Mejor calidad)
```cmd
ollama pull codellama:13b
```
- ‚úÖ Mejor calidad de c√≥digo
- ‚ö†Ô∏è M√°s lento
- ‚ö†Ô∏è Consume ~10GB RAM

#### **Descargar modelo de embeddings** (opcional pero recomendado)
```cmd
ollama pull nomic-embed-text
```
Este modelo permite a Continue entender mejor tu c√≥digo.

---

## üîß CONFIGURACI√ìN ACTUAL

Tu archivo `config.json` de Continue ya est√° configurado en:
```
C:\Users\luisr\AppData\Roaming\Code\User\globalStorage\continue.continue\config.json
```

**Configuraci√≥n actual:**
- **Modelo principal**: `deepseek-coder:6.7b`
- **Autocompletado**: `deepseek-coder:6.7b`
- **Embeddings**: `nomic-embed-text`
- **Servidor**: `http://localhost:11434`

---

## üìù C√ìMO USAR CONTINUE

### **1. Abrir Continue**
- Presiona `Ctrl + L` o haz clic en el √≠cono de Continue en la barra lateral

### **2. Chat con tu c√≥digo**
```
T√∫: Explica esta funci√≥n
T√∫: C√≥mo puedo mejorar este c√≥digo?
T√∫: Genera un README para este proyecto
```

### **3. Autocompletado**
- Empieza a escribir c√≥digo
- Continue sugerir√° c√≥digo autom√°ticamente
- Presiona `Tab` para aceptar

### **4. Seleccionar c√≥digo y preguntar**
1. Selecciona c√≥digo en el editor
2. Presiona `Ctrl + L`
3. Pregunta: "¬øQu√© hace este c√≥digo?"

### **5. Comandos personalizados**
- `/test` - Genera unit tests
- `/edit` - Modifica el c√≥digo seleccionado
- `/comment` - Agrega comentarios

---

## üîç VERIFICAR QUE TODO FUNCIONA

### **1. Verificar Ollama**
```cmd
# Ver si Ollama est√° corriendo
ollama list

# Deber√≠a mostrar los modelos descargados
# Ejemplo:
# NAME                      ID              SIZE
# deepseek-coder:6.7b      abc123          4.1GB
```

### **2. Probar el modelo**
```cmd
# Chat directo con el modelo
ollama run deepseek-coder:6.7b "Escribe una funci√≥n de fibonacci en JavaScript"
```

### **3. Probar Continue**
1. Abre cualquier archivo `.js` en tu proyecto
2. Presiona `Ctrl + L`
3. Pregunta: "¬øC√≥mo puedo mejorar mi c√≥digo?"
4. Deber√≠as ver una respuesta del modelo local

---

## üéØ COMPARACI√ìN: OLLAMA LOCAL vs COPILOT

| Caracter√≠stica | Ollama Local | GitHub Copilot |
|----------------|--------------|----------------|
| **Requiere internet** | ‚ùå No | ‚úÖ S√≠ |
| **Costo** | üí∞ Gratis | üí∞ $10/mes |
| **Privacidad** | üîí 100% local | ‚òÅÔ∏è En la nube |
| **Velocidad** | ‚ö° Depende de tu PC | ‚ö°‚ö° R√°pido |
| **Calidad** | ‚≠ê‚≠ê‚≠ê Buena | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente |
| **Uso de RAM** | üêè 4-10GB | üêè M√≠nimo |

---

## üö® SOLUCI√ìN DE PROBLEMAS

### **Problema: "Ollama no est√° corriendo"**
```cmd
# Verifica si Ollama est√° activo
ollama --version

# Si no funciona, reinicia el servicio
net start ollama
```

### **Problema: "El modelo es muy lento"**
- Cambia a un modelo m√°s peque√±o:
```cmd
ollama pull deepseek-coder:1.3b
```
- Actualiza `config.json` cambiando `deepseek-coder:6.7b` por `deepseek-coder:1.3b`

### **Problema: "Continue no responde"**
1. Verifica que Ollama est√© corriendo: `ollama list`
2. Reinicia VS Code
3. Abre la consola de Continue (√≠cono Continue ‚Üí ‚öôÔ∏è ‚Üí "Show Logs")

### **Problema: "No tengo suficiente RAM"**
```cmd
# Usa el modelo m√°s ligero
ollama pull deepseek-coder:1.3b

# O prueba con TinyLlama (muy ligero)
ollama pull tinyllama
```

---

## üìä MODELOS ALTERNATIVOS

Si `deepseek-coder` no te funciona bien, prueba estos:

### **Para JavaScript/Frontend:**
```cmd
ollama pull codellama:7b
```

### **Para Python:**
```cmd
ollama pull codellama:13b-python
```

### **Modelo general (no solo c√≥digo):**
```cmd
ollama pull mistral:7b
```

### **Modelo s√∫per ligero (4GB RAM):**
```cmd
ollama pull tinyllama
```

---

## üéì MEJORES PR√ÅCTICAS

### **1. Para proyectos peque√±os:**
- Usa `deepseek-coder:1.3b` (r√°pido y eficiente)

### **2. Para an√°lisis complejo:**
- Usa `codellama:13b` si tienes RAM suficiente

### **3. Para ahorrar bater√≠a:**
- Desactiva el autocompletado cuando no lo uses:
  ```json
  "tabAutocompleteModel": null
  ```

### **4. Para mejor contexto:**
- Descarga `nomic-embed-text` (mejora la comprensi√≥n del c√≥digo)

---

## üì± ATAJOS DE TECLADO

| Atajo | Acci√≥n |
|-------|--------|
| `Ctrl + L` | Abrir Continue chat |
| `Ctrl + I` | Edit inline (modificar c√≥digo) |
| `Ctrl + Shift + R` | Refactorizar c√≥digo |
| `Tab` | Aceptar sugerencia de autocompletado |
| `Esc` | Rechazar sugerencia |

---

## üîÑ ACTUALIZAR MODELOS

```cmd
# Actualizar un modelo existente
ollama pull deepseek-coder:6.7b

# Eliminar un modelo que no uses
ollama rm deepseek-coder:1.3b

# Ver todos los modelos instalados
ollama list
```

---

## üéØ CONFIGURACI√ìN AVANZADA

### **Cambiar modelo seg√∫n el lenguaje**

Edita `config.json` para usar diferentes modelos:

```json
{
  "models": [
    {
      "title": "DeepSeek (JavaScript)",
      "provider": "ollama",
      "model": "deepseek-coder:6.7b"
    },
    {
      "title": "CodeLlama (Python)",
      "provider": "ollama",
      "model": "codellama:13b-python"
    }
  ]
}
```

### **Activar/Desactivar telemetr√≠a**
Ya est√° desactivada en tu config:
```json
"allowAnonymousTelemetry": false
```

---

## üìö RECURSOS ADICIONALES

- **Ollama**: https://ollama.ai
- **Continue**: https://continue.dev
- **Modelos disponibles**: https://ollama.ai/library
- **Comunidad Continue**: https://discord.gg/continue

---

## ‚úÖ CHECKLIST DE INSTALACI√ìN

- [ ] Ollama instalado
- [ ] PC reiniciado
- [ ] Modelo descargado (`ollama pull deepseek-coder:6.7b`)
- [ ] Embeddings descargado (`ollama pull nomic-embed-text`)
- [ ] Continue configurado (ya hecho ‚úÖ)
- [ ] Probado con `Ctrl + L`
- [ ] Autocompletado funciona con `Tab`

---

## üéâ SIGUIENTE PASO

**Despu√©s de reiniciar tu PC, ejecuta:**

```cmd
ollama pull deepseek-coder:6.7b
ollama pull nomic-embed-text
```

**Luego:**
1. Abre VS Code
2. Presiona `Ctrl + L`
3. Pregunta: "Hola, ¬øpuedes ayudarme con mi c√≥digo?"

**¬°Y listo! Tendr√°s tu asistente de IA local funcionando** üöÄ

---

**Creado para**: Jorge Luis Risso Patr√≥n (@Luisitorisso)  
**Fecha**: Noviembre 2025  
**Versi√≥n**: 1.0
